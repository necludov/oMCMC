import numpy as np
import torch
import torch.nn.functional as F

from torch import distributions


class Normal:
    def __init__(self, mu, sigma, device):
        self.sigma = sigma
        self.mu = mu
        assert self.mu.dim() == 1
        self.dim = self.mu.shape[0]
        self.device = mu.device

    def log_prob(self, x):
        assert (x.shape[1] == self.dim)
        return -0.5 * torch.sum((x - self.mu.view([1,-1])) ** 2, dim=1) / self.sigma ** 2 

    def sample(self, n):
        eps = torch.empty([n, self.dim], device=self.device).normal_()
        return self.mu.view([1,-1]) + self.sigma * eps


class Banana:
    def __init__(self, device):
        self.dim = 2
        self.var = torch.tensor([10.0, 1.0], device=device)
        self.device = device

    def log_prob(self, x):
        log_prob1 = -0.5*x[:,0]**2/self.var[0]
        log_prob2 = -0.5*(x[:,1]-0.03*(x[:,0]**2-100.))**2/self.var[1]
        return log_prob1+log_prob2
    
    def mean(self):
        return np.array([0.0, -2.7])
    
    def std(self):
        return np.array([3.1609, 1.0867])


class ICG:
    def __init__(self, dim, device):
        self.dim = dim
        self.variances = 10**torch.linspace(-2.0, 2.0, self.dim, device=device)
        self.device = device

    def log_prob(self, x):
        assert x.shape[1] == self.dim
        return -0.5*torch.sum(x**2/self.variances.reshape([1,-1]), dim=1)

    def mean(self):
        return np.zeros(self.dim)

    def std(self):
        return np.sqrt(self.variances.numpy())


class MOGTwo:
    def __init__(self, device):
        self.dim = 2
        self.var = 0.5
        self.cov_matrix = torch.from_numpy(self.var * np.eye(2)).to(device).float()
        self.mean1 = torch.tensor([0, 0], device=device, dtype=torch.float)
        self.mean2 = torch.tensor([5, 0], device=device, dtype=torch.float)
        self.device = device

    def log_prob(self, x):
        prob1 = torch.exp(-torch.sum((x - self.mean1) ** 2, dim=1))
        prob2 = torch.exp(-torch.sum((x - self.mean2) ** 2, dim=1))
        return torch.log(prob1 + prob2+1e-10)

    def mean(self):
        return (0.5*(self.mean1+self.mean2)).cpu().numpy()

    def std(self):
        mu = self.mean()
        variance = self.var + 0.5*(self.mean1.cpu().numpy()**2 + self.mean2.cpu().numpy()**2) - mu**2
        return np.sqrt(variance)


class BayesianLogisticRegression:
    def __init__(self, data, labels, device):
        self.data = torch.tensor(data).to(device).float()
        self.labels = torch.tensor(labels).to(device).float().flatten()
        self.num_features = self.data.shape[1]
        self.dim = self.num_features + 1
        self.device = device

    def view_params(self, v):
        w = v[:,:self.num_features].view([-1, self.num_features, 1])
        b = v[:,self.num_features:].view([-1, 1, 1])
        return w, b

    def energy(self, v):
        w, b = self.view_params(v)
        x = self.data
        y = self.labels.view([1,-1,1])
        logits = torch.matmul(x,w) + b
        probs = torch.sigmoid(logits)
        nll = -y*torch.log(probs + 1e-16) - (1.0-y)*torch.log(1.0-probs  + 1e-16)
        nll = torch.sum(nll, dim=[1,2])
        negative_logprior = torch.sum(0.5*w**2, dim=[1,2])
        return negative_logprior + nll

    def log_prob(self, v):
        return -self.energy(v)


class Australian(BayesianLogisticRegression):
    def __init__(self, device):
        data = np.load('../data/australian/data.npy')
        labels = np.load('../data/australian/labels.npy')

        dm = np.mean(data, axis=0)
        ds = np.std(data, axis=0)
        data = (data - dm) / ds

        super(Australian, self).__init__(data, labels, device)

    def mean(self):
        return np.array([
            0.00573914,  0.01986144, -0.15868089,  0.36768475,  0.72598995,  0.08102263,
            0.25611847,  1.68464095,  0.19636668,  0.65685423, -0.14652498,  0.15565136,
            -0.32924402,  1.6396836,  -0.31129081])

    def std(self):
        return np.array([
            0.12749956,  0.13707998,  0.13329148,  0.12998348,  0.14871537,  0.14387384,
            0.16227234,  0.14832425,  0.16567627,  0.26399282,  0.12827283,  0.12381153,
            0.14707848,  0.56716324,  0.15749387])


class German(BayesianLogisticRegression):
    def __init__(self, device):
        data = np.load('../data/german/data.npy')
        labels = np.load('../data/german/labels.npy')

        dm = np.mean(data, axis=0)
        ds = np.std(data, axis=0)
        data = (data - dm) / ds

        super(German, self).__init__(data, labels, device)
        
    def mean(self):
        return np.array([
            -0.73619639,  0.419458, -0.41486377,  0.12679717, -0.36520298, -0.1790139,
            -0.15307771,  0.01321516,  0.18079792, - 0.11101034, - 0.22463548,  0.12258933,
            0.02874339, -0.13638893, -0.29289896,  0.27896283, -0.29997425,  0.30485174,
            0.27133239,  0.12250612, -0.06301813, -0.09286941, -0.02542205, -0.02292937,
            -1.20507437])

    def std(self):
        return np.array([
            0.09370191,  0.1066482,   0.097784,    0.11055009,  0.09572253,  0.09415687,
            0.08297686,  0.0928196,   0.10530122,  0.09953667,  0.07978824,  0.09610339,
            0.0867488,   0.09550436,  0.11943925,  0.08431934,  0.10446487,  0.12292658,
            0.11318609,  0.14040756,  0.1456459,   0.09062331,  0.13020753,  0.12852231,
            0.09891565])


class Heart(BayesianLogisticRegression):
    def __init__(self, device):
        data = np.load('../data/heart/data.npy')
        labels = np.load('../data/heart/labels.npy')

        dm = np.mean(data, axis=0)
        ds = np.std(data, axis=0)
        data = (data - dm) / ds

        super(Heart, self).__init__(data, labels, device)
        
    def mean(self):
        return np.array([
            -0.13996868,  0.71390106,  0.69571619,  0.43944853,  0.36997702, -0.27319424,
            0.31730518, -0.49617367,  0.40516419, 0.4312388,   0.26531786, 1.10337417,
            0.70054367, -0.25684964])

    def std(self):
        return np.array([
            0.22915648,  0.24545612,  0.20457998,  0.20270157,  0.21040644,  0.20094482,
            0.19749419,  0.24134014,  0.20230987,  0.25595334,  0.23709087,  0.24735325,
            0.20701178,  0.19771984])
    
class IRT:
    def __init__(self, device):
        self.dim = 501
        self.N = 30105
        self.device = device
        self.generate_data()
        
    def generate_data(self):
        self.alpha = torch.empty([100], device=self.device).normal_()
        self.beta = torch.empty([400], device=self.device).normal_()
        self.delta = 0.75+torch.empty([1], device=self.device).normal_()
        self.a_ids = torch.empty([self.N], device=self.device).uniform_()*len(self.alpha)
        self.a_ids = self.a_ids.long()
        self.b_ids = torch.empty([self.N], device=self.device).uniform_()*len(self.beta)
        self.b_ids = self.b_ids.long()
        probs = torch.sigmoid(self.alpha[self.a_ids]-self.beta[self.b_ids]+self.delta)
        self.y = (torch.empty([self.N], device=self.device).uniform_() > probs).long()
        
    def view_params(self, params):
        assert params.shape[1] == self.dim
        alpha = params[:,:len(self.alpha)]
        beta = params[:,len(self.alpha):len(self.alpha)+len(self.beta)]
        delta = params[:,len(self.alpha)+len(self.beta):]
        return alpha, beta, delta
        
    def log_prior(self, params):
        alpha, beta, delta = self.view_params(params)
        log_prior = -0.5*(alpha**2).sum(1)-0.5*(beta**2).sum(1)-0.5*((delta-0.75)**2).sum(1)
        return log_prior
        
    def log_likelihood(self, params):
        batch_ids = params.shape[0]
        alpha, beta, delta = self.view_params(params)
        probs = torch.sigmoid(alpha[:,self.a_ids]-beta[:,self.b_ids]+delta)
        log_like = torch.log(probs+1e-10)*self.y.view([1,-1])+(1-self.y.view([1,-1]))*(torch.log(1.0-probs+1e-10))
        log_like = log_like.sum(1)
        return log_like
        
    def log_prob(self, params):
        return self.log_likelihood(params) + self.log_prior(params)
    
    def mean(self):
        return np.array([ 5.2137e-01,  3.3118e-01,  2.3860e+00,  1.7111e-01,  2.5878e-02,
          3.5297e-01,  5.4446e-01,  2.2410e-01,  1.0197e+00,  4.8333e-01,
          1.7091e+00, -2.0200e+00, -7.8524e-01,  8.0364e-02, -1.5292e+00,
         -1.1088e+00,  2.0646e-01, -1.1962e+00, -1.5226e+00, -9.0200e-01,
          4.6646e-01,  1.4898e+00,  4.5551e-01,  7.5581e-01, -1.4951e-01,
         -5.2846e-02, -2.3322e+00, -5.3901e-01, -1.7218e-01,  6.7215e-01,
         -1.0172e+00,  5.9288e-02,  5.0176e-01,  4.1212e-01,  7.1352e-01,
          5.6228e-01,  3.6040e-01,  3.1521e-01, -7.6361e-01, -7.8465e-01,
         -1.0513e-01, -4.9409e-01, -2.0100e+00, -5.6383e-01, -3.4722e-01,
         -8.9012e-01,  1.2536e+00,  3.9412e-01,  1.9679e-01, -2.7807e-01,
         -3.8729e-01, -2.7406e-01, -1.0047e+00,  1.0566e+00, -2.1460e-01,
         -6.8284e-01,  6.2477e-01,  4.2711e-01,  5.7489e-01, -1.0629e+00,
         -4.3866e-01, -5.6402e-01, -5.4124e-01,  1.3515e+00, -2.7091e-01,
          6.3204e-01, -1.2261e+00,  9.5057e-01,  7.4468e-01, -6.0634e-01,
         -7.2783e-01,  3.5122e-01,  4.4838e-01,  8.2389e-02, -1.2170e+00,
          1.5517e+00, -4.3082e-01, -1.9300e-01,  1.5201e-01,  5.1462e-01,
          3.8008e-01, -9.4659e-01,  3.2062e+00, -1.6471e-01, -3.8630e-01,
         -1.8972e+00, -6.0829e-02, -3.4912e-01,  2.1004e-03, -1.6173e-01,
         -1.1599e+00, -1.5708e+00, -4.0294e-01, -2.6781e-01, -5.9872e-01,
          1.0458e+00,  6.9715e-02,  1.4149e+00,  7.8131e-01,  4.0808e-01,
          1.4192e+00,  1.8084e-01, -7.3730e-01,  1.2019e+00, -1.2488e+00,
         -9.0612e-01, -4.6296e-01,  7.6676e-01, -7.6326e-02, -7.3919e-01,
          2.4618e-01,  4.4953e-01, -1.0352e+00, -7.8859e-01,  9.0139e-01,
          1.3492e-01,  4.2298e-01, -1.8716e+00, -3.5803e-01, -1.9851e+00,
         -5.6105e-01,  4.1956e-01,  4.2557e-01, -1.2917e-01,  2.7226e-01,
          1.3142e+00, -5.4818e-01,  3.0618e-01, -3.8645e-01, -8.5938e-01,
         -1.4866e+00, -8.7307e-02,  8.5158e-01,  1.3542e+00, -2.3495e+00,
          8.1769e-01,  8.3253e-02,  1.2632e+00, -1.8709e+00, -1.9944e+00,
          4.2774e-01, -5.5224e-02,  3.0682e-01, -7.1925e-03, -2.2038e+00,
          3.0947e-01, -3.4884e-01, -4.9523e-01,  1.0792e+00,  5.8872e-01,
         -1.7724e+00,  2.4134e-01,  9.6900e-01, -1.4048e+00, -1.1063e+00,
          1.2775e+00,  4.0762e-01, -4.8924e-02, -8.5156e-01,  1.7356e-02,
          1.4800e+00, -1.0364e+00, -5.6485e-01, -3.7382e-01, -1.8172e+00,
          9.5607e-01,  3.2192e-01,  6.2845e-03,  2.5975e-01,  5.6440e-02,
          6.1970e-01, -6.2222e-01, -4.1997e-01, -1.2481e+00,  5.4981e-01,
          1.2828e+00, -1.5216e+00,  1.3779e+00, -3.5894e-02,  1.2111e+00,
          4.1665e-01,  3.4438e-02,  1.0757e-01,  4.1101e-01, -4.0222e-01,
         -5.4267e-02,  1.5486e-01, -4.7078e-01, -2.0245e-01, -8.5296e-01,
          7.1302e-01,  4.5142e-01,  4.5292e-03, -3.1383e-01, -1.9732e+00,
          2.0082e-01,  1.3014e+00, -4.4701e-01,  1.4201e+00,  8.1522e-01,
         -2.2041e-01,  9.7284e-02, -1.4451e-01,  8.3606e-01, -3.3258e-01,
         -1.8776e-02, -6.5889e-01,  9.1272e-01,  1.2497e+00,  6.2368e-01,
          6.2115e-01,  3.6931e-01,  7.3990e-01, -3.4429e-01, -1.0158e+00,
          1.2796e+00, -7.3945e-01,  1.1101e+00,  6.2473e-01, -1.0111e-01,
         -7.9244e-01,  1.5432e-01,  2.8511e-01, -1.2871e+00, -5.2290e-01,
          3.2788e-02, -6.2175e-01,  2.9284e-01,  3.2998e-01, -9.9589e-01,
          7.6783e-01, -1.3069e+00,  1.2838e+00, -1.0343e+00,  4.5206e-01,
          3.8818e-01, -1.5520e+00, -9.2982e-01,  9.6312e-01, -1.5481e+00,
          5.5956e-01, -1.4077e+00, -1.9310e+00, -4.9691e-01, -6.5198e-01,
          8.5972e-01,  1.0959e+00,  1.0211e+00,  3.8468e-01, -1.2540e+00,
          4.5978e-01,  1.2513e+00,  5.8991e-01,  2.0535e-01,  7.3763e-01,
          1.4331e-01, -5.7964e-01,  1.0519e-01,  2.0840e-01,  2.9731e-01,
          6.0375e-01, -1.0991e-02,  1.2702e+00, -2.0443e+00,  4.5651e-01,
          4.5046e-01, -1.1998e+00, -6.0882e-01,  5.5746e-01,  2.8540e-01,
          8.3662e-01, -8.1892e-01,  5.3349e-01, -1.2650e+00, -1.4054e-01,
          6.4904e-01, -2.4583e+00,  8.9606e-01,  6.9043e-02, -1.0032e+00,
         -7.3141e-01, -3.0410e-01, -1.2930e+00,  8.4342e-01,  2.5911e-01,
          3.5066e-01,  2.6790e-02, -7.9800e-01, -2.6045e-01, -1.1055e+00,
          8.3778e-01, -4.4357e-01, -5.6232e-01,  1.7404e-01, -7.9143e-01,
          1.0925e+00,  4.7438e-01, -3.4275e-01, -1.0386e+00, -4.0224e-01,
          2.4451e-01, -1.6666e+00,  4.4536e-01, -1.3231e+00, -2.1080e-01,
         -2.8360e-01,  1.3117e+00,  1.1252e+00,  1.1790e+00, -1.0131e+00,
          9.3135e-01, -3.0455e-01,  5.9139e-01,  8.9388e-01,  4.7786e-01,
          1.0425e+00, -6.1375e-01,  3.6824e-01, -5.1880e-01,  5.8670e-01,
         -6.6111e-02,  1.0175e-01,  8.7241e-01,  1.9685e-01, -1.1846e+00,
          6.8998e-01, -1.2125e+00,  1.3468e-01,  5.9123e-01, -9.4485e-01,
          1.3101e+00,  7.5968e-01,  5.9329e-01,  3.2189e-01,  3.8707e-01,
          1.3891e+00,  1.4280e+00, -1.0797e+00, -8.8003e-01, -1.9553e-01,
          3.7560e-02, -1.3193e+00,  8.8040e-01, -1.2795e+00,  5.3315e-01,
         -2.4882e-01, -1.0493e+00, -1.2091e+00,  1.1524e-01, -2.1621e-01,
         -2.0796e-01,  1.4426e+00,  5.9266e-01,  8.4623e-01,  1.2275e+00,
         -1.3144e-02, -5.0903e-01, -1.4384e-01, -4.2554e-01,  4.3460e-02,
          6.5859e-01,  1.1210e-01,  9.7516e-01, -1.0262e+00, -1.2802e+00,
          1.3221e+00, -8.5182e-01,  7.9798e-01,  7.1750e-01,  4.1726e-01,
          1.4174e+00, -3.9403e-01, -9.4187e-01, -8.0209e-01,  1.0523e+00,
          5.4379e-01, -1.8292e-01,  8.8255e-01, -1.6649e-01, -6.3267e-01,
          1.0692e+00,  3.1424e-01, -3.9304e-01, -7.5356e-01,  9.2696e-01,
         -9.9254e-01, -4.3409e-01,  1.3124e+00,  4.2855e-01,  8.8795e-01,
         -3.0977e-01, -9.5011e-01, -6.9240e-01,  2.9782e-01,  1.0245e+00,
         -7.6916e-01, -7.3905e-02, -1.8218e+00,  1.4189e+00, -1.0720e+00,
         -7.5737e-01,  1.1334e+00, -1.6677e-01, -4.6117e-02,  1.3688e+00,
         -6.2375e-01,  4.8123e-01, -9.7445e-01,  1.4769e+00,  7.8140e-01,
          1.1251e+00,  1.9645e-01, -3.8526e-01,  7.8059e-01,  7.8510e-01,
          6.0100e-01,  2.9291e-01,  1.3897e+00, -1.4520e-01,  2.9893e-01,
          9.7612e-01,  4.3702e-01, -1.2291e+00,  1.4543e-01, -1.2170e+00,
         -5.4769e-01, -5.8805e-01, -8.1558e-01,  5.5571e-02,  4.9190e-01,
         -9.6042e-01,  5.5544e-01, -1.3903e+00, -1.1015e-01,  4.0161e-01,
         -1.1118e-01, -6.2457e-01,  8.8457e-01,  6.1698e-01,  9.5078e-01,
         -9.2753e-01,  1.7750e-01, -1.5586e+00,  9.8176e-01,  2.4513e-01,
          7.1131e-01,  6.5967e-01,  8.5340e-01,  8.4558e-01,  1.0888e+00,
          3.8390e-01,  1.2967e+00,  1.2158e-01,  1.4540e+00, -1.0020e+00,
         -9.6490e-01,  1.3561e+00,  1.0634e+00, -1.0115e+00, -4.5480e-01,
          1.4901e-01,  4.6210e-01, -1.4225e+00, -3.9586e-01, -1.0645e+00,
         -3.4056e-01,  7.2983e-01, -5.7721e-01,  7.9993e-01,  1.1781e+00,
          3.8832e-01,  3.2750e-01,  1.7062e-02,  1.1849e+00, -9.2622e-01,
         -3.8378e-01,  7.6034e-01, -1.0368e-01,  1.1230e+00,  1.2953e-01,
         -1.0865e+00, -2.6408e+00,  5.0480e-01,  2.1047e-01,  1.1475e+00,
          2.7142e-01, -8.1614e-01,  1.0669e+00,  6.8187e-01,  9.0086e-01,
          8.6636e-03, -3.2372e-01, -4.5347e-01, -1.0260e+00,  7.3868e-01,
         -8.1267e-01,  2.7775e-02, -1.3184e-01, -3.4649e-02, -4.9358e-01,
         -3.2139e+00])
    
    def std(self):
        return np.array([0.2356, 0.2408, 0.1652, 0.2516, 0.2638, 0.2376, 0.2268, 0.2515, 0.2081,
         0.2326, 0.1796, 0.5258, 0.3650, 0.2586, 0.4505, 0.4004, 0.2437, 0.3951,
         0.4529, 0.3607, 0.2329, 0.1871, 0.2354, 0.2172, 0.2912, 0.2654, 0.5721,
         0.3242, 0.2835, 0.2174, 0.3770, 0.2640, 0.2340, 0.2363, 0.2202, 0.2301,
         0.2574, 0.2396, 0.3480, 0.3668, 0.2707, 0.3123, 0.5219, 0.3245, 0.2972,
         0.3649, 0.1967, 0.2333, 0.2531, 0.2956, 0.3141, 0.2905, 0.3821, 0.2040,
         0.2824, 0.3216, 0.2350, 0.2290, 0.2242, 0.3630, 0.3136, 0.3226, 0.3160,
         0.1864, 0.2901, 0.2348, 0.3957, 0.1946, 0.2105, 0.3236, 0.3334, 0.2437,
         0.2489, 0.2796, 0.4308, 0.1872, 0.3187, 0.2895, 0.2627, 0.2308, 0.2474,
         0.3841, 0.1624, 0.2981, 0.3054, 0.5347, 0.2801, 0.3041, 0.2640, 0.2535,
         0.4022, 0.4541, 0.2942, 0.2971, 0.3380, 0.2016, 0.2577, 0.1905, 0.2126,
         0.2374, 0.6729, 0.4442, 0.3647, 0.6958, 0.3202, 0.3596, 0.3549, 0.5502,
         0.4797, 0.3797, 0.5224, 0.5650, 0.3472, 0.3781, 0.6156, 0.5231, 0.5667,
         0.2611, 0.4220, 0.3176, 0.4255, 0.5026, 0.5116, 0.4643, 0.4712, 0.6808,
         0.4310, 0.5163, 0.4517, 0.3541, 0.2992, 0.4223, 0.6298, 0.6780, 0.2635,
         0.6295, 0.4581, 0.6887, 0.2885, 0.2829, 0.5114, 0.4505, 0.5352, 0.5381,
         0.2994, 0.5233, 0.4039, 0.3800, 0.6107, 0.5642, 0.2800, 0.4826, 0.6164,
         0.3492, 0.3454, 0.6947, 0.4755, 0.4814, 0.3461, 0.4217, 0.6640, 0.3600,
         0.4080, 0.4083, 0.2807, 0.6117, 0.5819, 0.5468, 0.5251, 0.5350, 0.5581,
         0.3739, 0.4258, 0.3366, 0.5027, 0.6894, 0.2788, 0.6788, 0.4510, 0.6983,
         0.5210, 0.4929, 0.5268, 0.5183, 0.4271, 0.4211, 0.4788, 0.4303, 0.4215,
         0.3441, 0.5683, 0.5698, 0.4841, 0.3990, 0.2969, 0.5191, 0.6880, 0.4142,
         0.6697, 0.6220, 0.4217, 0.4962, 0.4882, 0.5450, 0.4118, 0.4496, 0.3873,
         0.6118, 0.6939, 0.5572, 0.5590, 0.4706, 0.5518, 0.4226, 0.3503, 0.6927,
         0.4187, 0.6106, 0.5622, 0.4903, 0.4275, 0.4816, 0.5215, 0.3257, 0.4141,
         0.4862, 0.4146, 0.5220, 0.5822, 0.3383, 0.6308, 0.3009, 0.6898, 0.3545,
         0.5686, 0.5072, 0.2970, 0.3468, 0.5424, 0.2804, 0.5106, 0.2957, 0.2449,
         0.4216, 0.3692, 0.6339, 0.6025, 0.6095, 0.5090, 0.3321, 0.4746, 0.6945,
         0.5547, 0.4839, 0.5484, 0.5289, 0.4365, 0.4505, 0.5306, 0.5232, 0.5630,
         0.4471, 0.6978, 0.3004, 0.5074, 0.5097, 0.3379, 0.3754, 0.5666, 0.5138,
         0.6165, 0.3791, 0.5672, 0.3579, 0.4180, 0.5600, 0.2606, 0.6271, 0.4819,
         0.3359, 0.4034, 0.4014, 0.3043, 0.6229, 0.5196, 0.5201, 0.4560, 0.4107,
         0.4260, 0.3287, 0.6236, 0.4556, 0.3990, 0.4824, 0.3673, 0.5967, 0.5056,
         0.4252, 0.3527, 0.3997, 0.5201, 0.3507, 0.5733, 0.3351, 0.4221, 0.4289,
         0.6834, 0.6115, 0.6013, 0.3440, 0.6082, 0.4352, 0.5650, 0.5426, 0.5799,
         0.6010, 0.3866, 0.5753, 0.4344, 0.5679, 0.4438, 0.5238, 0.5494, 0.4689,
         0.3326, 0.5620, 0.3021, 0.4729, 0.5607, 0.3779, 0.6893, 0.5478, 0.5556,
         0.5199, 0.5208, 0.6815, 0.6824, 0.3553, 0.3721, 0.4877, 0.4886, 0.3056,
         0.6229, 0.3333, 0.5041, 0.4293, 0.3602, 0.3590, 0.4450, 0.4174, 0.4168,
         0.6636, 0.5574, 0.6244, 0.6895, 0.4956, 0.3967, 0.4260, 0.4613, 0.4525,
         0.5490, 0.5276, 0.6256, 0.3558, 0.3282, 0.6764, 0.3624, 0.5581, 0.5549,
         0.5788, 0.6763, 0.4304, 0.3258, 0.3515, 0.5970, 0.5013, 0.4532, 0.6185,
         0.4277, 0.3602, 0.6107, 0.5160, 0.4212, 0.3578, 0.6238, 0.4016, 0.4243,
         0.6860, 0.5704, 0.6151, 0.4173, 0.3535, 0.4337, 0.5146, 0.5995, 0.3631,
         0.4513, 0.3045, 0.6744, 0.3558, 0.3867, 0.6049, 0.4570, 0.4775, 0.6728,
         0.3840, 0.5090, 0.3466, 0.6648, 0.5511, 0.7017, 0.5189, 0.4031, 0.5580,
         0.6291, 0.5156, 0.4767, 0.6811, 0.4550, 0.5218, 0.6142, 0.5045, 0.3421,
         0.4751, 0.3235, 0.3837, 0.3746, 0.3458, 0.4406, 0.5694, 0.3735, 0.5037,
         0.3452, 0.4426, 0.5145, 0.4839, 0.4019, 0.6128, 0.5544, 0.5388, 0.3564,
         0.4858, 0.2912, 0.6086, 0.5191, 0.5591, 0.6434, 0.6220, 0.6233, 0.5997,
         0.5683, 0.5856, 0.4768, 0.6596, 0.3505, 0.3537, 0.6839, 0.6040, 0.3246,
         0.3993, 0.5233, 0.5216, 0.3259, 0.3997, 0.3596, 0.4598, 0.6352, 0.3899,
         0.5482, 0.7017, 0.5089, 0.4755, 0.4465, 0.7028, 0.3504, 0.4105, 0.6313,
         0.4876, 0.6034, 0.4723, 0.3214, 0.2277, 0.5067, 0.4707, 0.7008, 0.5116,
         0.4116, 0.6063, 0.6303, 0.6176, 0.4797, 0.3984, 0.4173, 0.3460, 0.5551,
         0.3557, 0.4493, 0.4494, 0.4619, 0.4238, 0.1165])
